{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import exp, tanh, log, cosh\n",
    "\n",
    "def cost_func(activation, target):\n",
    "    activation=np.asarray(activation)\n",
    "    target=np.asarray(target)\n",
    "    return 0.5*np.sum((target-activation)^2)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return exp(x)/(exp(x)+1.0)/(exp(x)+1.0)\n",
    "\n",
    "def reLU(x):\n",
    "    return max(0.0,x)\n",
    "\n",
    "def d_reLU(x):\n",
    "    return 0.5 * (np.sign(x) + 1)\n",
    "\n",
    "def softplus(x):\n",
    "    return log(1.0+exp(x))\n",
    "\n",
    "def d_softplus(x):\n",
    "    return 1.0/(1.0+exp(-x))\n",
    "\n",
    "def d_tanh(x):\n",
    "    return 4.0*cosh(x)*cosh(x)/(cosh(2.0*x)+1)/(cosh(2.0*x)+1)\n",
    "    \n",
    "class neuron:\n",
    "    def __init__(self, n_weights, activation_type='sigmoid', bias=0):   #supports linear, reLU, softplus, or sigmoid activations\n",
    "        self.n_weights=np.array(n_weights)\n",
    "        self.activation_type=activation_type\n",
    "        self.bias=bias\n",
    "        self.weights=np.random.rand(n_weights)\n",
    "        \n",
    "    def change_weights(self, weights_vector):\n",
    "        weights_vector=list(weights_vector)\n",
    "        if len(weights_vector)==self.n_weights:\n",
    "            self.weights=np.array(weights_vector)\n",
    "        else:\n",
    "            print 'Failed to change neuron connection strengths, weights vector has incorrect format'\n",
    "            \n",
    "    def print_weights(self):\n",
    "        print self.weights\n",
    "        \n",
    "    def activate(self, input_vector):\n",
    "        input_vector=np.array(input_vector)\n",
    "        if input_vector.size==self.n_weights:\n",
    "            input_vector=np.array(input_vector)\n",
    "            if self.activation_type=='sigmoid':\n",
    "                self.output=sigmoid(sum(self.weights*input_vector))+self.bias\n",
    "                return self.output\n",
    "            elif self.activation_type=='tanh':\n",
    "                self.output=tanh(sum(self.weights*input_vector))+self.bias\n",
    "                return self.output\n",
    "            elif self.activation_type=='reLU':\n",
    "                self.output=reLU(sum(self.weights*input_vector))+self.bias\n",
    "                return self.output\n",
    "            elif self.activation_type=='linear':\n",
    "                self.output=float(sum(self.weights*input_vector))+self.bias\n",
    "                return self.output\n",
    "            elif self.activation_type=='softplus':\n",
    "                self.output=softplus(sum(self.weights*input_vector))+self.bias\n",
    "                return self.output\n",
    "            else:\n",
    "                print 'Unknown activation function: '+self.activation_type\n",
    "        else:\n",
    "            print 'Activation input format incorrect'\n",
    "            \n",
    "    def backprop(self, input_vector, step_size=0.01):\n",
    "        if self.activation_type=='sigmoid':\n",
    "            self.gradient=self.weights*d_sigmoid(sum(self.weights*input_vector))\n",
    "        elif self.activation_type=='tanh':\n",
    "            self.gradient=self.weights*d_tanh(sum(self.weights*input_vector))\n",
    "        elif self.activation_type=='reLU':\n",
    "            self.gradient=self.weights*d_reLU(sum(self.weights*input_vector))\n",
    "        elif self.activation_type=='linear':\n",
    "            self.gradient=self.weights*sum(self.weights*input_vector)\n",
    "        elif self.activation_type=='softplus':\n",
    "            self.gradient=self.weights*d_softplus(sum(self.weights*input_vector))\n",
    "        else:\n",
    "            print 'Unknown activation function: '+self.activation_type\n",
    "            return 0\n",
    "        self.weights=self.weights+step_size*self.gradient\n",
    "        return self.weights\n",
    "\n",
    "class layer(neuron):\n",
    "    def __init__(self, n_neurons, n_weights_per_neuron, activation_type='sigmoid', bias_vector=0):\n",
    "        self.n_neurons=n_neurons\n",
    "        self.activation_type=activation_type\n",
    "        self.n_weights_per_neuron=n_weights_per_neuron\n",
    "        if bias_vector==0:\n",
    "            bias_vector=np.zeros(n_neurons)  \n",
    "        if len(bias_vector)!=n_neurons:\n",
    "            print 'Layer initialization failed, check length of bias vector'        \n",
    "        i=0\n",
    "        self.layer=[]\n",
    "        while(i<n_neurons):\n",
    "            self.layer.append(neuron(n_weights_per_neuron, activation_type, bias_vector[i]))\n",
    "            i+=1            \n",
    "        #print 'Layer created'\n",
    "    \n",
    "    def change_weights(self, weights_vector):\n",
    "        weights_vector=list(weights_vector)\n",
    "        if len(weights_vector)==self.n_neurons and len(weights_vector[0])==self.n_weights_per_neuron:\n",
    "            for i in range(len(self.layer)):                \n",
    "                self.layer[i].change_weights(weights_vector[i])\n",
    "        else:\n",
    "            print 'Failed to change layer connection strengths, weights vector has incorrect format'\n",
    "        \n",
    "    def print_weights(self):\n",
    "        for i in self.layer:\n",
    "            print i.weights\n",
    "            \n",
    "    def activate(self, input_vector):\n",
    "        input_vector=np.array(input_vector)\n",
    "        output=[]\n",
    "        if input_vector.ndim==2:\n",
    "            if input_vector.shape[0]!=self.n_neurons or input_vector.shape[1]!=self.n_weights_per_neuron:\n",
    "                print 'Incorrect input vector in layer activation function'\n",
    "                return output\n",
    "        elif input_vector.ndim!=self.n_weights_per_neuron:\n",
    "            print 'Incorrect input vector in layer activation function'\n",
    "            return output\n",
    "        for n in range(len(self.layer)):\n",
    "            output.append(self.layer[n].activate(input_vector[n]))\n",
    "        return output\n",
    "    \n",
    "class net():\n",
    "    n_layers=0\n",
    "    def __init__(self, connection_type='full'):\n",
    "        self.connection_type=connection_type\n",
    "        self.net=[]\n",
    "        #print 'Network created'\n",
    "    \n",
    "    def addlayer(self, layer):\n",
    "        if len(self.net)==0:\n",
    "            if layer.n_weights_per_neuron!=1:\n",
    "                print 'Addlayer failed, input layer should be added first with 1 weight per neuron'\n",
    "                return 0\n",
    "            else:\n",
    "                self.net.append(layer)\n",
    "                self.n_layers+=1\n",
    "        else:\n",
    "            i=len(self.net)\n",
    "            if self.connection_type=='full' and layer.n_weights_per_neuron==self.net[i-1].n_neurons:                   \n",
    "                self.net.append(layer)\n",
    "                self.n_layers+=1\n",
    "            else:\n",
    "                print 'Number of inputs per neuron is incorrect in the added layer, layer not added'         \n",
    "        \n",
    "    def activate(self, input_vector):\n",
    "        if len(input_vector)!=self.net[0].n_neurons:\n",
    "            print 'Activation failed. Input vector of incorrect length'\n",
    "            return 0\n",
    "        if self.net[0].n_weights_per_neuron!=1:\n",
    "            print 'Activation failed. Input layer should have 1 weight per neuron!'\n",
    "            return 0\n",
    "        out=self.net[0].activate(input_vector)\n",
    "        for i in range(1,len(self.net)):\n",
    "            inp=[]\n",
    "            for n in range(self.net[i].n_neurons):\n",
    "                inp.append(out)\n",
    "            out=self.net[i].activate(inp)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-106.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_func(220,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.553721594994\n",
      "[ 0.62072812  0.67006527]\n",
      "[ 0.6269354   0.67676592]\n",
      "0.559258810944\n"
     ]
    }
   ],
   "source": [
    "l1=layer(10, 1, activation_type='linear')\n",
    "\n",
    "l2=layer(200, 10, activation_type='sigmoid')\n",
    "\n",
    "l3=layer(100, 200, activation_type='sigmoid')\n",
    "\n",
    "l4=layer(1, 100, activation_type='linear')\n",
    "\n",
    "n=net()\n",
    "n.addlayer(l1)\n",
    "n.addlayer(l2)\n",
    "n.addlayer(l3)\n",
    "n.addlayer(l4)\n",
    "\n",
    "#print n.activate([1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "n1=neuron(2,activation_type='reLU')\n",
    "print n1.activate([1,-0.1])\n",
    "n1.print_weights()\n",
    "n1.backprop([1,-0.1], step_size=0.01)\n",
    "n1.print_weights()\n",
    "print n1.activate([1,-0.1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
