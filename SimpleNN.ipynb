{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import exp, tanh, log\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+exp(-x))\n",
    "\n",
    "def reLU(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def softplus(x):\n",
    "    return log(1+exp(x))\n",
    "    \n",
    "class neuron:\n",
    "    def __init__(self, n_weights, activation_type='sigmoid', bias=0):   #supports linear, reLU, softplus, or sigmoid activations\n",
    "        self.n_weights=np.array(n_weights)\n",
    "        self.activation_type=activation_type\n",
    "        self.bias=bias\n",
    "        self.weights=np.random.rand(n_weights) \n",
    "        \n",
    "    def change_weights(self, weights_vector):\n",
    "        weights_vector=list(weights_vector)\n",
    "        if len(weights_vector)==self.n_weights:\n",
    "            self.weights=weights_vector\n",
    "        else:\n",
    "            print 'Failed to change neuron connection strengths, weights vector has incorrect format'\n",
    "            \n",
    "    def print_weights(self):\n",
    "        print self.weights\n",
    "        \n",
    "    def activate(self, input_vector):\n",
    "        input_vector=np.array(input_vector)\n",
    "        if input_vector.size==self.n_weights:\n",
    "            input_vector=np.array(input_vector)\n",
    "            if self.activation_type=='sigmoid':\n",
    "                return sigmoid(sum(self.weights*input_vector))+self.bias\n",
    "            if self.activation_type=='tanh':\n",
    "                return tanh(sum(self.weights*input_vector))+self.bias\n",
    "            if self.activation_type=='reLU':\n",
    "                return reLU(sum(self.weights*input_vector))+self.bias\n",
    "            if self.activation_type=='linear':\n",
    "                return float(sum(self.weights*input_vector))+self.bias\n",
    "            else:\n",
    "                print 'Unknown activation function: '+self.activation_type\n",
    "        else:\n",
    "            print 'Activation input format incorrect'\n",
    "            \n",
    "class layer(neuron):\n",
    "    def __init__(self, n_neurons, n_weights_per_neuron, activation_type='sigmoid', bias_vector=0):\n",
    "        self.n_neurons=n_neurons\n",
    "        self.activation_type=activation_type\n",
    "        self.n_weights_per_neuron=n_weights_per_neuron\n",
    "        if bias_vector==0:\n",
    "            bias_vector=np.zeros(n_neurons)  \n",
    "        if len(bias_vector)!=n_neurons:\n",
    "            print 'Layer initialization failed, check length of bias vector'        \n",
    "        i=0\n",
    "        self.layer=[]\n",
    "        while(i<n_neurons):\n",
    "            self.layer.append(neuron(n_weights_per_neuron, activation_type, bias_vector[i]))\n",
    "            i+=1            \n",
    "        #print 'Layer created'\n",
    "    \n",
    "    def change_weights(self, weights_vector):\n",
    "        weights_vector=list(weights_vector)\n",
    "        if len(weights_vector)==self.n_neurons and len(weights_vector[0])==self.n_weights_per_neuron:\n",
    "            for i in range(len(self.layer)):                \n",
    "                self.layer[i].change_weights(weights_vector[i])\n",
    "        else:\n",
    "            print 'Failed to change layer connection strengths, weights vector has incorrect format'\n",
    "        \n",
    "    def print_weights(self):\n",
    "        for i in self.layer:\n",
    "            print i.weights\n",
    "            \n",
    "    def activate(self, input_vector):\n",
    "        input_vector=np.array(input_vector)\n",
    "        output=[]\n",
    "        if input_vector.ndim==2:\n",
    "            if input_vector.shape[0]!=self.n_neurons or input_vector.shape[1]!=self.n_weights_per_neuron:\n",
    "                print 'Incorrect input vector in layer activation function'\n",
    "                return output\n",
    "        elif input_vector.ndim!=self.n_weights_per_neuron:\n",
    "            print 'Incorrect input vector in layer activation function'\n",
    "            return output\n",
    "        for n in range(len(self.layer)):\n",
    "            output.append(self.layer[n].activate(input_vector[n]))\n",
    "        return output\n",
    "    \n",
    "class net():\n",
    "    n_layers=0\n",
    "    def __init__(self, connection_type='full'):\n",
    "        self.connection_type=connection_type\n",
    "        self.net=[]\n",
    "        #print 'Network created'\n",
    "    \n",
    "    def addlayer(self, layer):\n",
    "        if len(self.net)==0:\n",
    "            if layer.n_weights_per_neuron!=1:\n",
    "                print 'Addlayer failed, input layer should be added first with 1 weight per neuron'\n",
    "                return 0\n",
    "            else:\n",
    "                self.net.append(layer)\n",
    "                self.n_layers+=1\n",
    "        else:\n",
    "            i=len(self.net)\n",
    "            if self.connection_type=='full' and layer.n_weights_per_neuron==self.net[i-1].n_neurons:                   \n",
    "                self.net.append(layer)\n",
    "                self.n_layers+=1\n",
    "            else:\n",
    "                print 'Number of inputs per neuron is incorrect in the added layer, layer not added'         \n",
    "        \n",
    "    def activate(self, input_vector):\n",
    "        if len(input_vector)!=self.net[0].n_neurons:\n",
    "            print 'Activation failed. Input vector of incorrect length'\n",
    "            return 0\n",
    "        if self.net[0].n_weights_per_neuron!=1:\n",
    "            print 'Activation failed. Input layer should have 1 weight per neuron!'\n",
    "            return 0\n",
    "        out=self.net[0].activate(input_vector)\n",
    "        for i in range(1,len(self.net)):\n",
    "            inp=[]\n",
    "            for n in range(self.net[i].n_neurons):\n",
    "                inp.append(out)\n",
    "            out=self.net[i].activate(inp)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50.078147196944229]\n"
     ]
    }
   ],
   "source": [
    "l1=layer(3, 1, activation_type='linear')\n",
    "\n",
    "l2=layer(100, 3, activation_type='sigmoid')\n",
    "\n",
    "l3=layer(100, 100, activation_type='sigmoid')\n",
    "\n",
    "l4=layer(1, 100, activation_type='linear')\n",
    "\n",
    "n=net()\n",
    "n.addlayer(l1)\n",
    "n.addlayer(l2)\n",
    "n.addlayer(l3)\n",
    "n.addlayer(l4)\n",
    "\n",
    "print n.activate([1,1,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
